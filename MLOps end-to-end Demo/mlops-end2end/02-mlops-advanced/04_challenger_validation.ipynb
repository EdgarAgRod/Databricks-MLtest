{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2273ea32-44d2-4d80-af25-ebb436006bf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Challenger model validation\n",
    "\n",
    "This notebook performs validation tasks on the candidate __Challenger__ model.\n",
    "\n",
    "It goes through a few steps to validate the model before labelling it (by setting its alias) to `Challenger`.\n",
    "\n",
    "When organizations first start to put MLOps processes in place, they should consider having a \"human-in-the-loop\" to perform visual analyses to validate models before promoting them. As they get more familiar with the process, they can consider automating the steps in a __Workflow__ . The benefits of automation is to ensure that these validation checks are systematically performed before new models are integrated into inference pipelines or deployed for realtime serving. Of course, organizations can opt to retain a \"human-in-the-loop\" in any part of the process and put in place the degree of automation that suits its business needs.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/product/mlops/advanced/banners/mlflow-uc-end-to-end-advanced-4.png?raw=true\" width=\"1200\">\n",
    "\n",
    "*Note: in a typical mlops setup, this would run as part of an automated job to validate new model. For this demo, we'll run it as an interactive notebook.*\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection or disable tracker during installation. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-science&org_id=1832744760933926&notebook=%2F02-mlops-advanced%2F04_challenger_validation&demo_name=mlops-end2end&event=VIEW&path=%2F_dbdemos%2Fdata-science%2Fmlops-end2end%2F02-mlops-advanced%2F04_challenger_validation&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8afbe5af-6c69-4033-8804-7ff149004041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": null
    }
   },
   "source": [
    "### A cluster has been created for this demo\n",
    "To run this demo, just select the cluster `dbdemos-mlops-end2end-edgar_aguilerarod` from the dropdown menu ([open cluster configuration](https://dbc-07122dbb-1c85.cloud.databricks.com/#setting/clusters/0102-173414-9ev1v92w/configuration)). <br />\n",
    "*Note: If the cluster was deleted after 30 days, you can re-create it with `dbdemos.create_cluster('mlops-end2end')` or re-install the demo: `dbdemos.install('mlops-end2end')`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b69e36af-c895-4448-8a6a-5d64e6ad24c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## General Validation Checks\n",
    "\n",
    "<!--img style=\"float: right\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/churn-mlflow-webhook-1.png\" width=600 -->\n",
    "\n",
    "In the context of MLOps, there are more tests than simply how accurate a model will be.  To ensure the stability of our ML system and compliance with any regulatory requirements, we will subject each model added to the registry to a series of validation checks.  These include, but are not limited to:\n",
    "<br>\n",
    "* __Model documentation__\n",
    "* __Inference on production data__\n",
    "* __Champion-Challenger testing to ensure that business KPIs are acceptable__\n",
    "\n",
    "In this notebook we explore some approaches to performing these tests, and how we can add metadata to our models with tagging if they have passed a given test or not.\n",
    "\n",
    "This part is typically specific to your line of business and quality requirements.\n",
    "\n",
    "For each test, we'll add information using tags to know what has been validated in the model. We can also add Comments to a model if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0970be72-3e93-4cc4-b53e-77b418ff9744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet mlflow==2.14.3\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8db80104-f1f5-414d-9c0d-8ca4c0dacfaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $adv_mlops=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3cf3c9a3-4b92-4bc3-b4f1-948db3d4416a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Fetch Model information\n",
    "\n",
    "We will fetch the model information for the __Challenger__ model from Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f97dab99-8b5c-42ac-ab6f-afa15e8928de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fully qualified model name\n",
    "model_name = f\"{catalog}.{db}.advanced_mlops_churn\"\n",
    "\n",
    "# We are interested in validating the Challenger model\n",
    "model_alias = \"Challenger\"\n",
    "\n",
    "client = MlflowClient()\n",
    "model_details = client.get_model_version_by_alias(model_name, model_alias)\n",
    "model_version = int(model_details.version)\n",
    "run_info = client.get_run(run_id=model_details.run_id)\n",
    "\n",
    "print(f\"Validating {model_alias} model for {model_name} on model version {model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0abbb6dd-82bc-4c59-9e3f-1671bfa294f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d5fe396-f6ed-409f-9f1b-51485de3a576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Description check\n",
    "\n",
    "Has the data scientist provided a description of the model being submitted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd445025-bc26-4205-804d-2dc8a02a7386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# If there's no description or an insufficient number of charaters, tag accordingly\n",
    "if not model_details.description:\n",
    "  has_description = False\n",
    "  print(\"Please add model description\")\n",
    "elif not len(model_details.description) > 20:\n",
    "  has_description = False\n",
    "  print(\"Please add detailed model description (40 char min).\")\n",
    "else:\n",
    "  has_description = True\n",
    "\n",
    "print(f'Model {model_name} version {model_details.version} has description: {has_description}')\n",
    "client.set_model_version_tag(name=model_name, version=str(model_details.version), key=\"has_description\", value=has_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb24d649-1d14-4b4a-8b32-504b0007b66b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Validate prediction\n",
    "\n",
    "We want to test to see that the model can predict on production data.  So, we will load the model and the latest from the feature store and test making some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aec29bb3-d985-4ee9-a45e-3a0cf8d6c435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.feature_engineering import FeatureEngineeringClient\n",
    "from pyspark.sql.types import StructType\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "fe = FeatureEngineeringClient()\n",
    "\n",
    "# Load model as a Spark UDF\n",
    "model_uri = f\"models:/{model_name}@{model_alias}\"\n",
    "label_col = \"churn\"\n",
    "\n",
    "# Predict on a Spark DataFrame\n",
    "try:\n",
    "  # Read labels and IDs\n",
    "  labelsDF = spark.read.table(\"advanced_churn_label_table\")\n",
    "\n",
    "  # Batch score\n",
    "  features_w_preds = fe.score_batch(df=labelsDF, model_uri=model_uri, result_type=labelsDF.schema[label_col].dataType)\n",
    "  display(features_w_preds)\n",
    "  client.set_model_version_tag(name=model_name, version=str(model_version), key=\"predicts\", value=True)\n",
    "\n",
    "except Exception as e:\n",
    "  print(e)\n",
    "  features_w_preds = spark.createDataFrame([], StructType([]))\n",
    "  print(\"Unable to predict on features.\")\n",
    "  client.set_model_version_tag(name=model_name, version=str(model_version), key=\"predicts\", value=False)\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44b55701-10fb-4761-930b-ba33749b07db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Artifact check\n",
    "Has the data scientist logged supplemental artifacts along with the original model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f7246e87-9eeb-4516-b1e8-803bacf22665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create local directory\n",
    "local_dir = \"/tmp/model_artifacts\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.mkdir(local_dir)\n",
    "\n",
    "# Download artifacts from tracking server - no need to specify DBFS path here\n",
    "local_path = mlflow.artifacts.download_artifacts(run_id=run_info.info.run_id, dst_path=local_dir)\n",
    "\n",
    "# Tag model version as possessing artifacts or not\n",
    "if not os.listdir(local_path):\n",
    "  client.set_model_version_tag(name=model_name, version=model_version, key=\"has_artifacts\", value=False)\n",
    "  print(\"There are no artifacts associated with this model.  Please include some data visualization or data profiling.  MLflow supports HTML, .png, and more.\")\n",
    "\n",
    "else:\n",
    "  client.set_model_version_tag(name=model_name, version=str(model_version), key = \"has_artifacts\", value = True)\n",
    "  print(\"Artifacts downloaded in: {}\".format(local_path))\n",
    "  print(\"Artifacts: {}\".format(os.listdir(local_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6645e7d9-b19c-4f69-9687-03bdc582e159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Model performance metric\n",
    "\n",
    "We want to validate the model performance metric. Typically, we want to compare this metric obtained for the `@Challenger` model against that of the `@Champion` model. Since we have yet to register a `@Champion` model, we will only retrieve the metric for the `@Challenger` model without doing a comparison.\n",
    "\n",
    "The registered model captures information about the MLflow experiment run, where the model metrics were logged during training. This gives traceability from the deployed model back to the initial training runs.\n",
    "\n",
    "Here, we will use the F1 score for the out-of-sample test data that was set aside at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4adc9249-e567-4d5c-ad0c-2754026f0932",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_run_id = model_details.run_id\n",
    "f1_score = mlflow.get_run(model_run_id).data.metrics['test_f1_score']\n",
    "\n",
    "try:\n",
    "    #Compare the challenger f1 score to the existing champion if it exists\n",
    "    champion_model = client.get_model_version_by_alias(model_name, \"Champion\")\n",
    "    champion_f1 = mlflow.get_run(champion_model.run_id).data.metrics['test_f1_score']\n",
    "    print(f'Champion f1 score: {champion_f1}. Challenger f1 score: {f1_score}.')\n",
    "    metric_f1_passed = f1_score >= champion_f1\n",
    "\n",
    "except:\n",
    "    print(f\"No Champion found. Accept the model as it's the first one.\")\n",
    "    metric_f1_passed = True\n",
    "\n",
    "print(f'Model {model_name} version {model_details.version} metric_f1_passed: {metric_f1_passed}')\n",
    "\n",
    "# Tag that F1 metric check has passed\n",
    "client.set_model_version_tag(name=model_name, version=model_details.version, key=\"metric_f1_passed\", value=metric_f1_passed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e1fe1ef-db78-4fbc-af49-5ed25e4dafd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Benchmark or business metrics on the eval dataset\n",
    "\n",
    "Let's use our validation dataset to check the potential new model impact.\n",
    "\n",
    "***Note: This is just to evaluate our models, not to be confused with A/B testing**. A/B testing is done online, splitting the traffic to 2 models and requires a feedback loop to evaluate the effect of the prediction (e.g. after a prediction, did the discount we offered to the customer prevent the churn?). We will cover A/B testing later in the real-time model serving notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc088c30-f372-41aa-b63b-0291049b83fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Get our validation dataset:\n",
    "validation_df = spark.table('advanced_churn_label_table').filter(\"split='validate'\")\n",
    "\n",
    "# Call the model with the given alias and return the prediction\n",
    "def predict_churn(validation_df, model_alias):\n",
    "    features_w_preds = fe.score_batch(df=validation_df, model_uri=f\"models:/{model_name}@{model_alias}\", \n",
    "                                      result_type=validation_df.schema[label_col].dataType)\n",
    "\n",
    "    return features_w_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6c683ba-20b3-4d3c-b751-ab78fa320171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Note: this is over-simplified and depends on the use-case, but the idea is to evaluate our model against business metrics\n",
    "cost_of_customer_churn = 2000 #in dollar\n",
    "cost_of_discount = 500 #in dollar\n",
    "\n",
    "cost_true_negative = 0 #did not churn, we did not give him the discount\n",
    "cost_false_negative = cost_of_customer_churn #did churn, we lost the customer\n",
    "cost_true_positive = cost_of_customer_churn -cost_of_discount #We avoided churn with the discount\n",
    "cost_false_positive = -cost_of_discount #doesn't churn, we gave the discount for free\n",
    "\n",
    "def get_model_value_in_dollar(model_alias):\n",
    "    # Convert preds_df to Pandas DataFrame\n",
    "    model_predictions = predict_churn(validation_df, model_alias).toPandas()\n",
    "    # Calculate the confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(model_predictions['churn'], model_predictions['prediction']).ravel()\n",
    "    return tn * cost_true_negative+ fp * cost_false_positive + fn * cost_false_negative + tp * cost_true_positive\n",
    "\n",
    "try:\n",
    "    champion_model = client.get_model_version_by_alias(model_name, \"Champion\")\n",
    "    champion_potential_revenue_gain = get_model_value_in_dollar(\"Champion\")\n",
    "    challenger_potential_revenue_gain = get_model_value_in_dollar(\"Challenger\")\n",
    "\n",
    "    data = {'Model Alias': ['Challenger', 'Champion'],\n",
    "            'Potential Revenue Gain': [challenger_potential_revenue_gain, champion_potential_revenue_gain]}\n",
    "\n",
    "except:\n",
    "    print(\"No Champion found. Skipping business metrics evaluation.\")\n",
    "    print(\"You can return to re-run this cell after promoting the Challenger model as Champion in the rest of this notebook.\")\n",
    "\n",
    "    data = {'Model Alias': ['Challenger', 'Champion'],\n",
    "            'Potential Revenue Gain': [0, 0]}\n",
    "\n",
    "# Create a bar plot using plotly express\n",
    "px.bar(data, x='Model Alias', y='Potential Revenue Gain', color='Model Alias',\n",
    "    labels={'Potential Revenue Gain': 'Revenue Impacted'},\n",
    "    title='Business Metrics - Revenue Impacted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96159290-eaab-4da9-9af1-93c80d9bd8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Validation results\n",
    "\n",
    "That's it! We have demonstrated some simple checks on the model. Let's take a look at the validation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17b1509f-9d9d-44d1-a1d7-04a70f9da5dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = client.get_model_version(model_name, model_version)\n",
    "results.tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c1ed293-eb82-48b3-8849-c0b11052d7c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Promoting the Challenger to Champion\n",
    "\n",
    "When we are satisfied with the results of the __Challenger__ model, we can then promote it to Champion. This is done by setting its alias to `@Champion`. Inference pipelines that load the model using the `@Champion` alias will then be loading this new model. The alias on the older Champion model, if there is one, will be automatically unset. The model retains its `@Challenger` alias until a newer Challenger model is deployed with the alias to replace it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1192e15-78ad-499f-b433-42637bda77de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if results.tags[\"has_description\"] and results.tags[\"metric_f1_passed\"] and results.tags['predicts']:\n",
    "  print(f\"Registering model {model_name} Version {model_version} as Champion!\")\n",
    "  client.set_registered_model_alias(\n",
    "    name=model_name,\n",
    "    alias=\"Champion\",\n",
    "    version=model_version\n",
    "  )\n",
    "else:\n",
    "  raise Exception(\"Model not ready for promotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c453c072-d3fd-418c-8812-ccaf05900cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that we are promoting the model while keeping in one catalog and schema in this demo. We do this for simplicity so that the demo can be self-contained to a catalog and schema.\n",
    "\n",
    "In actual practice, it is recommended to maintain separate catalogs for Dev, QA and Prod data and AI assets. This applies to models as well. In that case, we would register the production model to a production catalog, with an appropriate `@alias` set for it. This can be done programatically, and triggered when the model is ready to be promoted to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f45a1a15-f7c4-4cc6-86d5-eb964e241239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Congratulations, our model is now validated and promoted accordingly\n",
    "\n",
    "We now have the certainty that our model is ready to be used in inference pipelines and in realtime serving endpoints, as it matches our validation standards.\n",
    "\n",
    "\n",
    "Next: [Run batch inference from our newly promoted Champion model]($./05_batch_inference)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_challenger_validation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
